{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import and devide dataset\n",
    "from numpy import *\n",
    "from sklearn import preprocessing\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_x, train_y = load_svmlight_file(\"a9a.txt\", n_features=123)\n",
    "test_x, test_y = load_svmlight_file(\"a9atest.t\", n_features=123)\n",
    "train_x =  train_y.reshape(train_y.shape[0],1)\n",
    "test_x =  test_y.reshape(test_y.shape[0],1)\n",
    "train_y[train_y == -1] = 0\n",
    "test_y[test_y == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize parameters of model\n",
    "import numpy as np\n",
    "m, n = np.shape(train_x)\n",
    "w = np.ones((n, 1))   # weight \n",
    "r = 0.05   # learning rate\n",
    "maxIters = 40   # epoch times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def cal_loss(x,y,w):\n",
    "    return  -( y*log(sigmoid(x * w)) + (1-y)*log(1-sigmoid(x * w)) ).sum()/ x.shape[0]\n",
    "def sigmoid(a):\n",
    "    return 1/(1+exp(-a))\n",
    "def cal_accur(x,y,w):\n",
    "    h =sigmoid( x * w)\n",
    "    h[h > 0.5] = 1\n",
    "    h[h <= 0.5] = 0\n",
    "    return np.count_nonzero(h==y)/ x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cal_gradient_sgd(k):\n",
    "    random_num = random.randint(0,m)\n",
    "    return (train_x[random_num].T * (sigmoid(train_x[random_num] * k) - train_y[random_num]))\n",
    "\n",
    "train_loss, evaluation_loss,train_accr,evaluation_accr = [],[],[],[]\n",
    "\n",
    "def SGD(w):\n",
    "    for i in range(0, maxIters):\n",
    "        grad = cal_gradient_sgd(w)\n",
    "        w -= r * grad         \n",
    "        train_loss.append(cal_loss(train_x,train_y,w))\n",
    "        evaluation_loss.append(cal_loss(test_x,test_y,w))\n",
    "        train_accr.append(cal_accur(train_x,train_y,w))\n",
    "        evaluation_accr.append(cal_accur(test_x,test_y,w))\n",
    "SGD(w)\n",
    "\n",
    "# Draw graphs\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.plot(train_loss, label=\"train_loss\")\n",
    "plt.plot(evaluation_loss,label=\"evaluation_loss\" )\n",
    "plt.legend(loc ='upper right')\n",
    "\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(train_accr, label=\"train_accuracy\")\n",
    "plt.plot(evaluation_accr, label=\"evaluation_accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NAG \n",
    "train_loss_nag,evaluation_loss_nag,train_accr_nag,evaluation_accr_nag = [],[],[],[]\n",
    "w = np.ones((n, 1))\n",
    "\n",
    "def NAG(w):\n",
    "    gamma = 0.9\n",
    "    vt = 0\n",
    "    for i in range(0, maxIters):\n",
    "        grad = cal_gradient_sgd(w - gamma*vt)\n",
    "        vt = gamma*vt + r * grad\n",
    "        w-=vt\n",
    "        train_loss_nag.append(cal_loss(train_x,train_y,w))\n",
    "        evaluation_loss_nag.append(cal_loss(test_x,test_y,w))\n",
    "        train_accr_nag.append(cal_accur(train_x,train_y,w))\n",
    "        evaluation_accr_nag.append(cal_accur(test_x,test_y,w))\n",
    "        \n",
    "NAG(w)\n",
    "'''\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(train_loss_nag, label=\"train_loss\")\n",
    "plt.plot(evaluation_loss_nag,label=\"evaluation_loss\" )\n",
    "plt.legend(loc ='upper right')\n",
    "\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(train_accr_nag, label=\"train_accuracy\")\n",
    "plt.plot(evaluation_accr_nag, label=\"evaluation_accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loss_RMSProp,evaluation_loss_RMSProp,train_accr_RMSProp,evaluation_accr_RMSProp = [],[],[],[]\n",
    "w = np.ones((n, 1))\n",
    "\n",
    "def RMSProp(w):\n",
    "    gamma = 0.9\n",
    "    vt = 0\n",
    "    Egt = 0\n",
    "    e=0.00000001  \n",
    "    \n",
    "    learning_rate = 0.3\n",
    "\n",
    "    for i in range(0, maxIters):\n",
    "        grad = cal_grad_sgd(w - gamma*vt)\n",
    "        Egt = gamma * Egt + ((1-gamma)*(grad**2)).sum()  \n",
    "        w = w - learning_rate*grad/math.sqrt(Egt + e)  \n",
    "\n",
    "        train_loss_RMSProp.append(cal_loss(train_x,train_y,w))\n",
    "        evaluation_loss_RMSProp.append(cal_loss(test_x,test_y,w))\n",
    "        train_accr_RMSProp.append(cal_accur(train_x,train_y,w))\n",
    "        evaluation_accr_RMSProp.append(cal_accur(test_x,test_y,w))\n",
    "        \n",
    "RMSProp(w)\n",
    "'''\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(train_loss_RMSProp, label=\"train_loss\")\n",
    "plt.plot(evaluation_loss_RMSProp,label=\"evaluation_loss\" )\n",
    "plt.legend(loc ='upper right')\n",
    "\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(train_accr_RMSProp, label=\"train_accuracy\")\n",
    "plt.plot(evaluation_accr_RMSProp, label=\"evaluation_accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AdaDelta\n",
    "train_loss_adaDelta,evaluation_loss_adaDelta,train_accr_adaDelta,evaluation_accr_adaDelta = [],[],[],[]\n",
    "w = np.ones((n, 1))\n",
    "\n",
    "def adaDelta(w):\n",
    "\n",
    "    rho = 0.9\n",
    "    Egt=0  \n",
    "    Edt = 0  \n",
    "    e=0.00000001  \n",
    "    delta = 0  \n",
    "    learning_rate = 2000\n",
    "\n",
    "    for i in range(0, maxIters):\n",
    "        \n",
    "        grad = cal_grad_sgd(w)\n",
    "        Egt = rho * Egt + ((1-rho)*(grad**2) ).sum()  \n",
    "        delta = - math.sqrt(Edt + e)*grad/math.sqrt(Egt + e)\n",
    "        Edt =rho*Edt+( (1-rho)*(delta**2) ).sum()  \n",
    "        w = w + learning_rate*delta \n",
    "        \n",
    "        train_loss_adaDelta.append(cal_loss(train_x,train_y,w))\n",
    "        evaluation_loss_adaDelta.append(cal_loss(test_x,test_y,w))\n",
    "        train_accr_adaDelta.append(cal_accur(train_x,train_y,w))\n",
    "        evaluation_accr_adaDelta.append(cal_accur(test_x,test_y,w))\n",
    "        \n",
    "adaDelta(w)\n",
    "'''\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(train_loss_adaDelta, label=\"train_loss\")\n",
    "plt.plot(evaluation_loss_adaDelta,label=\"evaluation_loss\" )\n",
    "plt.legend(loc ='upper right')\n",
    "\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(train_accr_adaDelta, label=\"train_accuracy\")\n",
    "plt.plot(evaluation_accr_adaDelta, label=\"evaluation_accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loss_adam,evaluation_loss_adam,train_accr_adam,evaluation_accr_adam = [],[],[],[]\n",
    "w = np.ones((n, 1))\n",
    "\n",
    "def adam(w):\n",
    " \n",
    "    t = 0 \n",
    "    m = 0  \n",
    "    v = 0  \n",
    "    b1 = 0.9  \n",
    "    b2 = 0.995  \n",
    "    learning_rate = 0.05\n",
    "    \n",
    "    for i in range(0, maxIters):\n",
    "        \n",
    "        grad = cal_grad_sgd(w)\n",
    "        t +=1 \n",
    "        m = b1*m + ((1-b1)*grad).sum() \n",
    "        v = b2*v + ((1-b2)*(grad**2)).sum()  \n",
    "        mt = m/(1-(b1**t))  \n",
    "        vt = v/(1-(b2**t)) \n",
    "        w = w- learning_rate * mt/(math.sqrt(vt)+e) \n",
    "        \n",
    "        train_loss_adam.append(cal_loss(train_x,train_y,w))\n",
    "        evaluation_loss_adam.append(cal_loss(test_x,test_y,w))\n",
    "        train_accr_adam.append(cal_accur(train_x,train_y,w))\n",
    "        evaluation_accr_adam.append(cal_accur(test_x,test_y,w))\n",
    "        \n",
    "adam(w)\n",
    "'''\n",
    "\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(train_loss_adam, label=\"train_loss\")\n",
    "plt.plot(evaluation_loss_adam,label=\"evaluation_loss\" )\n",
    "plt.legend(loc ='upper right')\n",
    "\n",
    "\n",
    "'''\n",
    "plt.plot(train_loss, label=\"train_loss\")\n",
    "plt.plot(train_loss_nag, label=\"train_loss_nag\")\n",
    "plt.plot(train_loss_adaDelta, label=\"train_loss_adaDelta\")\n",
    "plt.plot(train_loss_RMSProp, label =\"train_loss_RMSProp\")\n",
    "plt.plot(train_loss_adam, label=\"train_loss_adam\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
