{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import and devide dataset\n",
    "from numpy import *\n",
    "from sklearn import preprocessing\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "train_x, train_y = load_svmlight_file(\"a9a.txt\", n_features=123)\n",
    "test_x, test_y = load_svmlight_file(\"a9atest.t\", n_features=123)\n",
    "train_x =  train_y.reshape(train_y.shape[0],1)\n",
    "test_x =  test_y.reshape(test_y.shape[0],1)\n",
    "#train_y[train_y == -1] = 0\n",
    "#test_y[test_y == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize parameters of model\n",
    "import numpy as np\n",
    "m, n = np.shape(train_x)\n",
    "w = np.ones((n, 1))   # weight \n",
    "r = 0.01   # learning rate\n",
    "maxIters = 50  \n",
    "c = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "def cal_stochastic_gradient(w):\n",
    "    index =  (1 - train_y * (train_x * w) < 0)\n",
    "    y = train_y.copy()\n",
    "    y[index] = 0\n",
    "    randomNum = np.random.randint(0,train_x.shape[0])\n",
    "    epsilon_gradient = - ((train_x)[randomNum].T * y[randomNum]).reshape(123,1)\n",
    "    grad = w +  epsilon_gradient\n",
    "    return grad\n",
    "\n",
    "def cal_loss(x,y,w):\n",
    "    epsilon_loss = 1 - y * x.dot(w)\n",
    "    epsilon_loss[epsilon_loss<0] = 0\n",
    "    loss = 0.5 * np.dot(w.transpose(), w).sum() + epsilon_loss.sum()\n",
    "    return loss/x.shape[0]\n",
    "\n",
    "def cal_acc(x,y,w):\n",
    "    h =x*w\n",
    "    h[h > 0.5] = 1\n",
    "    h[h <= 0.5] = -1\n",
    "    return np.count_nonzero(h==y)/ x.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loss, evaluation_loss, train_accr, evaluation_accr = [],[],[],[]\n",
    "def gradientDescent(w):\n",
    "    for i in range(maxIters):\n",
    "        gradient = cal_stochastic_gradient(w)\n",
    "        W -= r*gradient    \n",
    "        train_loss.append(cal_loss(w,train_x,train_y))\n",
    "        evaluation_loss.append(cal_loss(w,test_x,test_y))\n",
    "        train_accr.append(cal_accur(train_x,train_y,w))\n",
    "        evaluation_accr.append(cal_accur(test_x,test_y,w))      \n",
    "gradientDescent(w)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot( train_loss, label=\"train_loss\")\n",
    "plt.plot( evaluation_loss,label=\"evaluation_loss\" )\n",
    "plt.legend(loc=\"upper right\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adam\n",
    "train_loss_adam,evaluation_loss_adam,train_accr_adam,evaluation_accr_adam = [],[],[],[]\n",
    "w = np.ones((n, 1))\n",
    "def adam(w):\n",
    "    t = 0 \n",
    "    m = 0  \n",
    "    v = 0  \n",
    "    b1 = 0.9  \n",
    "    b2 = 0.995  \n",
    "    learning_rate = 0.05\n",
    "    for i in range(0, maxIters):\n",
    "        grad = cal_grad_sgd(w)\n",
    "        t +=1 \n",
    "        m = b1*m + ((1-b1)*grad).sum() \n",
    "        v = b2*v + ((1-b2)*(grad**2)).sum()  \n",
    "        mt = m/(1-(b1**t))  \n",
    "        vt = v/(1-(b2**t)) \n",
    "        W -= learning_rate * mt/(math.sqrt(vt)+e)      \n",
    "        train_loss_adam.append(cal_loss(train_x,train_y,w))\n",
    "        evaluation_loss_adam.append(cal_loss(test_x,test_y,w))\n",
    "        train_accr_adam.append(cal_accur(train_x,train_y,w))\n",
    "        evaluation_accr_adam.append(cal_accur(test_x,test_y,w))     \n",
    "adam(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loss_RMSP,evaluation_loss_RMSP,train_accr_RMSP,evaluation_accr_RMSP = [],[],[],[]\n",
    "w = np.ones((n, 1))\n",
    "def RMSProp(w):\n",
    "    gamma = 0.9\n",
    "    vt = 0\n",
    "    Egt = 0\n",
    "    e=0.00000001      \n",
    "    learning_rate = 0.3\n",
    "    for i in range(0, maxIters):\n",
    "        gradient = cal_stochastic_gradient(w - gamma*vt)\n",
    "        Egt = gamma * Egt + ((1-gama)*(gradient**2)).sum()  \n",
    "        w -= learning_rate*gradient/math.sqrt(Egt + e)  \n",
    "        train_loss_RMSP.append(cal_loss(train_x,train_y,w))\n",
    "        evaluation_loss_RMSP.append(cal_loss(test_x,test_y,w))\n",
    "        train_accr_RMSP.append(cal_accur(train_x,train_y,w))\n",
    "        evaluation_accr_RMSP.append(cal_accur(test_x,test_y,w))          \n",
    "RMSProp(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loss_adaD,evaluation_loss_adaD,train_accr_adaD,evaluation_accr_adaD = [],[],[],[]\n",
    "w = np.ones((n, 1))\n",
    "def adaDelta(w):\n",
    "    gamma = 0.9\n",
    "    Egt=0  \n",
    "    Edt = 0  \n",
    "    e=0.00000001  \n",
    "    delta = 0  \n",
    "    learning_rate = 2000\n",
    "    for i in range(0, maxIters):      \n",
    "        gradient = cal_stochastic_gradient(w)\n",
    "        Egt = gamma * Egt + ((1-gamma)*(gradient**2) ).sum()  \n",
    "        delta = - math.sqrt(Edt + e)*gradient/math.sqrt(Egt + e)\n",
    "        Edt =gamma*Edt+( (1-gamma)*(delta**2) ).sum()  \n",
    "        w = w + learning_rate*delta     \n",
    "        train_loss_adaD.append(cal_loss(train_x,train_y,w))\n",
    "        evaluation_loss_adaD.append(cal_loss(test_x,test_y,w))\n",
    "        train_accr_adaD.append(cal_accur(train_x,train_y,w))\n",
    "        evaluation_accr_adaD.append(cal_accur(test_x,test_y,w))   \n",
    "adaDelta(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loss_nag,evaluation_loss_nag,train_accr_nag,evaluation_accr_nag = [],[],[],[]\n",
    "w = np.ones((n, 1))\n",
    "def NAG(w):\n",
    "    vt = 0\n",
    "    gamma = 0.9\n",
    "    for i in range(maxIters):\n",
    "        gradient = cal_stochastic_gradient(w -gamma*vt)\n",
    "        vt = gamma*vt + learning_rate * gradient\n",
    "        w -= vt\n",
    "        train_loss_nag.append(cal_loss(train_x,train_y,w))\n",
    "        evaluation_loss_nag.append(cal_loss(test_x,test_y,w))\n",
    "        train_accr_nag.append(cal_accur(train_x,train_y,w))\n",
    "        evaluation_accr_nag.append(cal_accur(test_x,test_y,w))     \n",
    "NAG(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(train_loss, label=\"loss\")\n",
    "plt.plot(train_loss_nag, label=\"loss_NAG\")\n",
    "plt.plot(train_loss_adaD, label=\"loss_adaDelta\")\n",
    "plt.plot(train_loss_RMSProp, label =\"loss_RMSProp\")\n",
    "plt.plot(train_loss_adam, label=\"loss_Adam\")\n",
    "plt.legend(loc=\"upper right\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
