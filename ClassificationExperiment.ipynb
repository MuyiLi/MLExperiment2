{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import and devide dataset\n",
    "from numpy import *\n",
    "from sklearn import preprocessing\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "train_x, train_y = load_svmlight_file(\"a9a.txt\", n_features=123)\n",
    "test_x, test_y = load_svmlight_file(\"a9atest.t\", n_features=123)\n",
    "train_x =  train_y.reshape(train_y.shape[0],1)\n",
    "test_x =  test_y.reshape(test_y.shape[0],1)\n",
    "#train_y[train_y == -1] = 0\n",
    "#test_y[test_y == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize parameters of model\n",
    "import numpy as np\n",
    "m, n = np.shape(train_x)\n",
    "w = np.ones((n, 1))   # weight \n",
    "r = 0.01   # learning rate\n",
    "maxIters = 50  \n",
    "c = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "def cal_stochastic_gradient(t):\n",
    "    index =  (1 - train_y * (train_x * w) < 0)\n",
    "    y = train_y.copy()\n",
    "    y[index] = 0\n",
    "    randomNum = np.random.randint(0,train_x.shape[0])\n",
    "    epsilon_gradient = - ((train_x)[randomNum].T * y[randomNum]).reshape(123,1)\n",
    "    grad = w +  epsilon_gradient\n",
    "    return grad\n",
    "\n",
    "def cal_loss(x,y,w):\n",
    "    epsilon_loss = 1 - y * x.dot(w)\n",
    "    epsilon_loss[epsilon_loss<0] = 0\n",
    "    loss = 0.5 * np.dot(w.transpose(), w).sum() + epsilon_loss.sum()\n",
    "    return loss/x.shape[0]\n",
    "\n",
    "def cal_acc(x,y,w):\n",
    "    h =x*w\n",
    "    h[h > 0.5] = 1\n",
    "    h[h <= 0.5] = -1\n",
    "    return np.count_nonzero(h==y)/ x.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f1eb08555662>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtrain_accr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcal_accur\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mevaluation_accr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcal_accur\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-f1eb08555662>\u001b[0m in \u001b[0;36mgradientDescent\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxIters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcal_stochastic_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mW\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcal_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-63e3618b57bb>\u001b[0m in \u001b[0;36mcal_stochastic_gradient\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mrandomNum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mepsilon_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandomNum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandomNum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "train_loss, evaluation_loss, train_accr, evaluation_accr = [],[],[],[]\n",
    "def gradientDescent(W):\n",
    "    for i in range(maxIters):\n",
    "        gradient = cal_stochastic_gradient(W)\n",
    "        W -= r*gradient    \n",
    "        train_loss.append(cal_loss(W,train_x,train_y))\n",
    "        evaluation_loss.append( cal_loss(W,test_x,test_y))\n",
    "        train_accr.append(cal_accur(train_x,train_y,w))\n",
    "        evaluation_accr.append(cal_accur(test_x,test_y,w))      \n",
    "gradientDescent(w)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot( train_loss, label=\"train_loss\")\n",
    "plt.plot( evaluation_loss,label=\"evaluation_loss\" )\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "train_loss_adam,evaluation_loss_adam,train_accr_adam,evaluation_accr_adam = [],[],[],[]\n",
    "w = np.ones((n, 1))\n",
    "\n",
    "def adam(w):\n",
    " \n",
    "    t = 0 \n",
    "    m = 0  \n",
    "    v = 0  \n",
    "    b1 = 0.9  \n",
    "    b2 = 0.995  \n",
    "    learning_rate = 0.05\n",
    "    \n",
    "    for i in range(0, maxIteration):\n",
    "        \n",
    "        grad = cal_grad_sgd(w)\n",
    "        t +=1 \n",
    "        m = b1*m + ((1-b1)*grad).sum() \n",
    "        v = b2*v + ((1-b2)*(grad**2)).sum()  \n",
    "        mt = m/(1-(b1**t))  \n",
    "        vt = v/(1-(b2**t)) \n",
    "        w = w- learning_rate * mt/(math.sqrt(vt)+e) \n",
    "        \n",
    "        train_loss_adam.append(cal_loss(train_x,train_y,w))\n",
    "        evaluation_loss_adam.append(cal_loss(test_x,test_y,w))\n",
    "        train_accr_adam.append(cal_accur(train_x,train_y,w))\n",
    "        evaluation_accr_adam.append(cal_accur(test_x,test_y,w))\n",
    "        \n",
    "adam(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
